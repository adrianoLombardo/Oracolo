from __future__ import annotations

import re
from datetime import datetime
import io
from pathlib import Path
from typing import Any, Dict, List, Tuple

import openai
from langdetect import DetectorFactory, detect
from .utils import retry_with_backoff

DetectorFactory.seed = 42

_IT_SW = {
    "di","e","che","il","la","un","una","non","per","con","come","sono","sei","Ã¨","siamo","siete","sono",
    "io","tu","lui","lei","noi","voi","loro","questo","questa","quello","quella","qui","lÃ¬","dove","quando",
    "perchÃ©","come","cosa","tutto","anche","ma","se","nel","nella","dei","delle","degli","agli","alle","allo",
    "fare","andare","venire","dire","vedere","puÃ²","posso","devo","voglio","grazie","ciao"
}
_EN_SW = {
    "the","and","to","of","in","that","it","is","you","i","we","they","this","these","those","for","with",
    "on","at","as","but","if","not","are","be","was","were","have","has","do","does","did","what","when",
    "where","why","how","all","also","can","could","should","would","hello","hi","thanks","please"
}


def _score_lang(text: str, lang: str, *, debug: bool = False) -> float:
    if not text:
        return 0.0
    toks = re.findall(r"[a-zÃ Ã¨Ã©Ã¬Ã²Ã³Ã¹]+", text.lower())
    if not toks:
        return 0.0
    sw = _IT_SW if lang == "it" else _EN_SW
    hits = sum(1 for t in toks if t in sw)
    coverage = hits / max(len(toks), 1)
    try:
        ld = detect(text)
        bonus = 0.5 if (ld == lang) else 0.0
    except Exception:
        bonus = 0.0
    length_bonus = min(len(toks), 30) / 30 * 0.2
    score = coverage + bonus + length_bonus
    if debug:
        short = (text[:80] + "â€¦") if len(text) > 80 else text
        print(f"   [DBG] {lang.upper()} score={score:.3f} text='{short}'")
    return score


def transcribe(audio: Path | bytes, client, stt_model: str, *, debug: bool = False) -> Tuple[str, str]:
    """Trascrivi ``audio`` restituendo testo e lingua stimata.

    Viene effettuata una chiamata "auto"; solo se il punteggio di lingua Ã¨
    basso si ripete la stessa richiesta forzando ``language=``.
    """

    def _call_transcription(**kwargs) -> str:
        def do_call() -> str:
            if isinstance(audio, (str, Path)):
                with open(audio, "rb") as f:
                    tx = client.audio.transcriptions.create(
                        model=stt_model, file=f, **kwargs
                    )
            else:
                bio = io.BytesIO(audio)
                tx = client.audio.transcriptions.create(
                    model=stt_model, file=bio, **kwargs
                )
            return (tx.text or "").strip()

        try:
            return retry_with_backoff(do_call)
        except openai.OpenAIError as e:
            print(f"Errore OpenAI: {e}")
            return ""

    print("ðŸ§  Trascrizione (auto)â€¦")
    text = _call_transcription(
        prompt=(
            "Language is either Italian or English. Focus on neuroscience, "
            "neuroaesthetics, contemporary art, the universe, and "
            "neuroscientific AI. Ignore any other language."
        )
    )

    s_it = _score_lang(text, "it", debug=debug)
    s_en = _score_lang(text, "en", debug=debug)
    lang = "it" if s_it >= s_en else "en"
    best = max(s_it, s_en)

    if best < 0.6:
        print(f"â†» Trascrizione forzata {lang.upper()}â€¦")
        prompt = (
            "Lingua: italiano. Dominio: neuroscienze, neuroestetica, "
            "arte contemporanea, universo e IA neuroscientifica."
            if lang == "it"
            else "Language: English. Domain: neuroscience, neuroaesthetics, "
            "contemporary art, universe, and neuroscientific AI."
        )
        text = _call_transcription(language=lang, prompt=prompt)
        best = _score_lang(text, lang, debug=debug)

    if best == 0:
        print("âš ï¸ Per favore parla in italiano o inglese.")
        return "", ""

    print("ðŸŒ Lingua rilevata: IT" if lang == "it" else "ðŸŒ Lingua rilevata: EN")
    return text, lang


def oracle_answer(
    question: str,
    lang_hint: str,
    client: Any,
    llm_model: str,
    style_prompt: str,
    *,
    context: List[Dict[str, Any]] | None = None,
    history: List[Dict[str, str]] | None = None,
    topic: str | None = None,
    policy_prompt: str = "",
    mode: str = "detailed",
) -> Tuple[str, List[Dict[str, Any]]]:
    """Return an answer generated by the LLM client.

    The function composes a conversation with optional context and history,
    augments it with simple policy instructions and attempts the request up to
    three times to handle transient API errors gracefully.
    """
    print("âœ¨ Interrogo lâ€™Oracoloâ€¦")

    lang_clause = "Answer in English." if lang_hint == "en" else "Rispondi in italiano."
    topic_clause = (
        " Rispondi solo con informazioni coerenti al topic corrente; non mescolare altri temi a meno che l'utente lo chieda esplicitamente. Topic: "
        + topic
        if topic
        else ""
    )
    mode_clause = (
        " Stile conciso: 2-4 frasi e termina con una domanda di follow-up."
        if mode == "concise"
        else " Struttura: 1) sintesi, 2) 2-3 dettagli puntuali, 3) fonti citate [1], [2], â€¦"
    )
    grounding_clause = (
        "Answer ONLY using the passages; if they are insufficient, ask for clarifications."
        if lang_hint == "en"
        else "Rispondi SOLO usando i passaggi; se non sono sufficienti, chiedi chiarimenti."
    )
    policy = (
        (policy_prompt or "")
        + topic_clause
        + " "
        + grounding_clause
        + " "
        + lang_clause
        + mode_clause
    )

    messages: List[Dict[str, str]] = []
    if style_prompt:
        messages.append({"role": "system", "content": style_prompt})
    if context:
        ctx_txt = "\n".join(
            f"[{i+1}] {c.get('text','')}" for i, c in enumerate(context) if c.get("text")
        )
        if ctx_txt:
            messages.append({"role": "system", "content": f"Fonti:\n{ctx_txt}"})
    if history:
        messages.extend(history)
    messages.append({"role": "user", "content": question})

    def do_request():
        return client.responses.create(
            model=llm_model,
            instructions=policy,
            input=messages,
        )

    try:
        resp = retry_with_backoff(do_request)
        ans = resp.output_text.strip()
        return ans, context or []
    except openai.OpenAIError as e:
        print(f"Errore OpenAI: {e}")
        return "", context or []


def synthesize(text: str, out_path: Path, client, tts_model: str, tts_voice: str) -> None:
    print("ðŸŽ§ Sintesi vocaleâ€¦")
    out_path.parent.mkdir(parents=True, exist_ok=True)

    def do_call() -> Path:
        try:
            with client.audio.speech.with_streaming_response.create(
                model=tts_model, voice=tts_voice, input=text, response_format="wav"
            ) as resp:
                resp.stream_to_file(out_path.as_posix())
            return out_path
        except TypeError:
            alt = out_path.with_suffix(".mp3") if out_path.suffix.lower() != ".mp3" else out_path
            with client.audio.speech.with_streaming_response.create(
                model=tts_model, voice=tts_voice, input=text
            ) as resp:
                resp.stream_to_file(alt.as_posix())
            return alt

    try:
        final_path = retry_with_backoff(do_call)
        print(f"âœ… Audio â†’ {final_path.name}")
        return
    except openai.OpenAIError as e:
        print(f"Errore OpenAI: {e}")
    print("âŒ Impossibile sintetizzare l'audio.")


def export_audio_answer(
    text: str,
    out_path: Path,
    *,
    synth: Any | None = None,
    client: Any | None = None,
    tts_model: str = "",
    tts_voice: str = "",
) -> None:
    """Export ``text`` as an audio file to ``out_path``.

    A custom ``synth`` callable can be provided for testing purposes. If not
    given, ``client``/``tts_model``/``tts_voice`` are used with
    :func:`synthesize` to generate the audio.
    """
    out_path.parent.mkdir(parents=True, exist_ok=True)
    if synth is not None:
        synth(text, out_path)
    else:
        if client is None:
            raise ValueError("client required if synth not provided")
        synthesize(text, out_path, client, tts_model, tts_voice)


def format_citations(sources: list[dict[str, str]]) -> str:
    """Return a comma-separated string of source identifiers."""
    return ", ".join(s.get("id", "") for s in sources if s.get("id"))


def append_log(
    q: str,
    a: str,
    log_path: Path,
    *,
    lang: str = "",
    topic: str | None = None,
    sources: list[dict[str, str]] | None = None,
) -> None:
    """Append a structured CSV line with optional metadata."""
    log_path.parent.mkdir(parents=True, exist_ok=True)
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def clean(s: str) -> str:
        return s.replace('"', "'")

    src_str = ";".join(
        f"{s.get('id','')}:{s.get('score',0):.2f}" for s in (sources or [])
    )
    line = (
        f'"{ts}","{lang}","{clean(topic or "")}",'
        f'"{clean(q)}","{clean(a)}","{src_str}"\n'
    )
    if not log_path.exists():
        log_path.write_text(
            '"timestamp","lang","topic","question","answer","sources"\n',
            encoding="utf-8",
        )
    with log_path.open("a", encoding="utf-8") as f:
        f.write(line)


